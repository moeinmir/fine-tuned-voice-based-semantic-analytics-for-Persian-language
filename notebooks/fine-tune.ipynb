{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning superb/wav2vec2-base-superb-er With Shemo Persian Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing Dependencies And Defining Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from transformers import AutoFeatureExtractor, Wav2Vec2ForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "CWD_PATH = os.getcwd()\n",
    "ROOT_PATH = os.path.abspath(os.path.join(CWD_PATH, '..'))\n",
    "UTILS_PATH = os.path.join(ROOT_PATH, 'utils')\n",
    "DATASETS_BASE_PATH = os.path.join(ROOT_PATH, 'data')\n",
    "RESULTS_PATH = os.path.join(ROOT_PATH,'results')\n",
    "if UTILS_PATH not in sys.path:\n",
    "    sys.path.append(UTILS_PATH)\n",
    "\n",
    "FEATURES_PATH = os.path.join(ROOT_PATH, 'features')\n",
    "MODELS_PATH = os.path.join(ROOT_PATH, 'models')\n",
    "os.makedirs(FEATURES_PATH, exist_ok=True)\n",
    "os.makedirs(MODELS_PATH, exist_ok=True)\n",
    "os.makedirs(RESULTS_PATH,exist_ok=True)\n",
    "MODEL_NAME = \"superb/wav2vec2-base-superb-er\"\n",
    "AUDIO_MAX_LENGTH = 8 \n",
    "SAMPLE_RATE = 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading The Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbk/.local/lib/python3.10/site-packages/transformers/configuration_utils.py:315: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "extractor = AutoFeatureExtractor.from_pretrained(MODEL_NAME)\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=4\n",
    ")\n",
    "model.gradient_checkpointing_enable()\n",
    "id2label = model.config.id2label\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Functions And Classes That Are Going To Be Used In Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_validate_data(basePath, representationFilePath):\n",
    "    with open(representationFilePath, 'r', encoding='utf-8') as f:\n",
    "        metadata = json.load(f)    \n",
    "    paths, labels = [], []\n",
    "    for fileName, details in metadata.items():\n",
    "        filePath = os.path.join(basePath, details[\"path\"])\n",
    "        if os.path.exists(filePath):\n",
    "            try:\n",
    "                librosa.load(filePath, sr=SAMPLE_RATE, duration=1)\n",
    "                paths.append(filePath)\n",
    "                labels.append(details[\"emotion\"].lower())                \n",
    "            except:\n",
    "                print(f\"Skipping corrupted file: {filePath}\")          \n",
    "    return pd.DataFrame({'speech': paths, 'label': labels})\n",
    "\n",
    "def extract_features(file_path):\n",
    "    try:\n",
    "        audio, _ = librosa.load(file_path, sr=SAMPLE_RATE, duration=AUDIO_MAX_LENGTH)\n",
    "        inputs = extractor(\n",
    "            audio,\n",
    "            sampling_rate=SAMPLE_RATE,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            max_length=SAMPLE_RATE*AUDIO_MAX_LENGTH,\n",
    "            truncation=True\n",
    "        )\n",
    "        return inputs.input_values[0].numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return np.zeros(SAMPLE_RATE*AUDIO_MAX_LENGTH)\n",
    "\n",
    "def precompute_and_save_features(df, save_path):\n",
    "    features = []\n",
    "    for path in tqdm(df['speech'], desc=\"Extracting features\"):\n",
    "        features.append(extract_features(path))\n",
    "    np.save(save_path, np.array(features))\n",
    "\n",
    "class AudioFeaturesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features_path, features):\n",
    "        self.features = np.load(features_path, mmap_mode='r')\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_values': torch.tensor(self.features[idx]),\n",
    "            'features': torch.tensor(self.features[idx])\n",
    "        }\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Shemo Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "shemo_dataset_audio_files_path = os.path.join(DATASETS_BASE_PATH,'shemo')\n",
    "shemo_dataset_representation_file_path = os.path.join(DATASETS_BASE_PATH, 'shemo/modified_shemo.json')\n",
    "shemo_df = load_and_validate_data(\n",
    "    shemo_dataset_audio_files_path,\n",
    "    shemo_dataset_representation_file_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping And Filtering Shemo DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2766\n"
     ]
    }
   ],
   "source": [
    "label_mapping = {'happiness': 'hap', 'anger': 'ang', 'sadness': 'sad', 'neutral': 'neu'}\n",
    "shemo_df = shemo_df[shemo_df['label'].isin(label_mapping.keys())].copy()\n",
    "shemo_df['label'] = shemo_df['label'].map(label_mapping)\n",
    "shemo_df['label_id'] = shemo_df['label'].map(label2id)\n",
    "print(len(shemo_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Data Frame For The Test And Train\n",
    "in here we used test size of 0.8, in train_test_split function so we could have a pretty smaller train dataset, so the ram can handle it, for the training process we give the model a dataset of 400 so the ram could handle it, the test dataset length is almost 500, for the testing we use all the remaining data and do it manually which is probably a good thing because we see if our model has the problem of over fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, rest_df = train_test_split(shemo_df,test_size=0.8 , random_state=42)\n",
    "test_df = rest_df.groupby('label').head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting And Saving Features For Train And Test Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing training features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 553/553 [00:01<00:00, 340.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing evaluation features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 400/400 [00:01<00:00, 325.91it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Precomputing training features...\")\n",
    "precompute_and_save_features(train_df, f\"{FEATURES_PATH}/shemo_train_features.npy\")\n",
    "print(\"Precomputing evaluation features...\")\n",
    "precompute_and_save_features(test_df, f\"{FEATURES_PATH}/shemo_test_features.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AudioFeaturesDataset(\n",
    "    f\"{FEATURES_PATH}/shemo_train_features.npy\",\n",
    "    train_df['label_id'].values\n",
    ")\n",
    "\n",
    "test_dataset = AudioFeaturesDataset(\n",
    "    f\"{FEATURES_PATH}/shemo_train_features.npy\",\n",
    "    test_df['label_id'].values\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_with_shemo_model_path = os.path.join(MODELS_PATH,'w2v_fine_tuned_with_shemo_voice_based_semantic_analytics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='417' max='417' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [417/417 1:11:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed successfully!\n",
      "Model saved successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"Training completed successfully!\")\n",
    "    model.save_pretrained(fine_tuned_with_shemo_model_path)\n",
    "    extractor.save_pretrained(fine_tuned_with_shemo_model_path)\n",
    "    print(\"Model saved successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Training failed: {str(e)}\")\n",
    "    print(\"Saving current progress...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2213it [10:20,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to /home/dbk/fine-tuned-voice-based-semantic-analytics-for-Persian-language/results/shemo_dataset_test_result.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(fine_tuned_with_shemo_model_path)\n",
    "extractor = AutoFeatureExtractor.from_pretrained(fine_tuned_with_shemo_model_path)\n",
    "\n",
    "id2label = model.config.id2label\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "def predict(audio_path):\n",
    "    audio, _ = librosa.load(audio_path, sr=SAMPLE_RATE, duration=8)\n",
    "    inputs = extractor(audio, sampling_rate=SAMPLE_RATE, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    return id2label[torch.argmax(logits).item()]\n",
    "\n",
    "number_of_correct_predictions = 0\n",
    "number_of_incorrect_predictions = 0\n",
    "prediction_results = []\n",
    "for index, row in tqdm(rest_df.iterrows()):\n",
    "    prediction = predict(row['speech'])\n",
    "\n",
    "    result = {\n",
    "        'file_path': row['speech'],\n",
    "        'predicted_label': prediction,\n",
    "        'correct_label': row['label'],\n",
    "        'correct': prediction == row['label']\n",
    "    }\n",
    "    if prediction == row['label']:\n",
    "        number_of_correct_predictions += 1\n",
    "    else:\n",
    "        number_of_incorrect_predictions += 1\n",
    "        prediction_results.append(result)\n",
    "results_df = pd.DataFrame(prediction_results)\n",
    "result_path = os.path.join(RESULTS_PATH,\"shemo_dataset_test_result.xlsx\")\n",
    "results_df.to_excel(result_path, index=False)\n",
    "print(f\"Results saved to {result_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Result\n",
      "number of correct predictions: 1872\n",
      "number of incorrect predictions: 341\n",
      "accuracy: 84.59 percent\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Result\")\n",
    "print(f\"number of correct predictions: {number_of_correct_predictions}\" )\n",
    "print(f\"number of incorrect predictions: {number_of_incorrect_predictions}\")\n",
    "accuracy   =   100*number_of_correct_predictions/(number_of_correct_predictions+number_of_incorrect_predictions)\n",
    "print(f\"accuracy: {accuracy:.2f} percent\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
